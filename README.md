# Deep Quadric Learning - A tale in 4 episodes

## Why quadric learning?
If digital technological evolution taught us one thing then it's this: Moore's 1st law will not always be on our side, his 2nd law never was.

The current hype concerning Large Language Models (LLM), the huge amount of data and parameters to build and train them clearly show that the trend of ever bigger data and ever bigger models can't be sustained and clearly negatively impacts

- available AI compute power
- AI costs
- the democratization of access to AI technology
- the AI carbon foot-print & sustainability of deep learning models

Everything that contributes positively in a meaningful way in order to improve the situation is absolutely crucial.

This project of Deep Quadric Learning wants to be a small and humble contribution to the above mentioned dilemma by introducing quadric decision hyperplanes and 2nd order separability and enable the study of its impact on model size, parameters and interconnectivity of deep learning models. In one of the later episodes you'll find references to past and current research in this field including my own.

While reducing net model size itself, quadric learning at the same time allows for various subsequent methods like hyper parameter optimization, model distillation, sparse activation, evolutionary architecture search etc. etc.

In 4 episodes this project tries to introduce the concept of quadric learning from the motivation (1) the single quadric neuron (2) to quadric layers (3) and lastly hybrid and purely quadric models (4).

The proof of concept implementation in PyTorch does in no way affect the applicability of Deep Quadric Learning to other frameworks like Tensor Flow etc.
